1、Sparse autoencoder network structure: Encoder-decoder symmetrical architecture. The encoder consists of 4 fully connected layers (input→ 1152 → 576 → 288 → 144), and the decoder and encoder are completely symmetrical. Activation function: Sigmoid. Optimizer: Adam. Learning rate (lr) : 0.01 (dynamically adjusted during training using ReduceLROnPlateau). Training epochs: 150. KL-divergence sparse constraint: The target sparsity p = 0.05. The weight coefficient β = 1e-7 (the constant BETA in the code). Whether to use sparse constraints: Enable (use_sparse=True). Loss function: Reconstruction loss (MSE) + sparsity constraint (KL divergence).

2、Base learner optimizer: Adam. Learning rate (lr) : 0.001-0.005 (set according to different sub-modules). dropout rate: GraphNetEncoder layer: 0.1. Global Attention layer: 0.5. Normalization: The combination of BatchNorm and LayerNorm. Attention mechanisms: multi-head attention (n_head=1-4), and global attention (GlobalAttn).
